%\section{Safety-aware apprenticeship learning algorithm}
%%\vspace{-2mm}
In this section, we introduce the Counterexample Guided Apprenticeship Learning (CEGAL)
algorithm to solve the SafeAL problem. 
It can be viewed as a special case of the safety-aware learning framework described in the previous section. 
In addition to combining policy verification, counterexample generation and AL, our approach uses an adaptive weighting scheme to weight the separation from $\mu_E$ with the separation from $\mu_{cex}$.
%\vspace{-4mm}
\begin{eqnarray}
&&\underset{\omega}{\max}\underset{\pi\in\Pi_S,\tilde{\pi}\in\Pi_S, cex\in CEX}{\min}\ \omega^T(k(\mu_E - \mu_{\pi})+(1-k)(\mu_{ \tilde{\pi}}  - \mu_{cex}))\label{eq:sec5_1}\\
&&s.t.\: ||\omega||_2\leq 1 \label{eq:sec5_4},\: k\in[0, 1]\label{eq:sec5_5} \nonumber\\ 
&&\quad\ \:\omega^T(\mu_E - \mu_{\pi})\leq\omega^T(\mu_E - \mu_{\pi'}),\ \forall\pi'\in\Pi_S \label{eq:sec5_2} \nonumber\\ 
&&\quad\ \:\omega^T(\mu_{\tilde{\pi}} - \mu_{cex})\leq\omega^T(\mu_{ \tilde{\pi}'} - \mu_{cex'}),\ \forall \tilde{\pi}'\in\Pi_S, \forall cex'\in{CEX} \nonumber
\end{eqnarray}
%\vspace{-3mm}
In essence, we take a weighted-sum approach for solving the multi-objective optimization problem  (\ref{eq:sec4_3}). Assuming that $\Pi_S=\{\pi_{1},$ $\pi_{2},$ $\pi_{3},$ $\ldots \}$ is a set of candidate policies that all satisfy $\Phi$, ${CEX} =\{cex_1,$ $cex_2,$ $cex_3,$ $\ldots\}$ is a set of counterexamples. We introduce a parameter $k$ and change (\ref{eq:sec4_3}) into a weighted sum optimization problem (\ref{eq:sec5_1}). Note that $\pi$ and $\tilde\pi$ can be different. The optimal $\omega$ solved from (\ref{eq:sec5_1}) can be used to generate a new policy $\pi_\omega$ by using algorithms such as policy iteration. 
We use a probabilistic model checker, such as PRISM~\cite{kwiatkowska2002prism}, to check if $\pi_\omega$ satisfies $\Phi$. If it does, then it will be added to $\Pi_S$. Otherwise, a counterexample generator, such as COMICS~\cite{DBLP:journals/corr/abs-1206-0603}, is used to generate a (minimal) counterexample $cex_{\pi_\omega}$, which will be added to ${CEX}$.  

%\vspace{-2mm}
\begin{algorithm}[htb]
\caption{Counterexample-Guided Apprenticeship Learning (CEGAL)}
\begin{algorithmic}[1]
\State \textbf{Input}:
\\\qquad $M\gets$ A partially known $MDP\backslash R$; $f\gets$ A vector of feature functions
\\\qquad $\mu_E\gets$ The expected features of expert trajectories $\{\tau_0, \tau_1, \ldots,\tau_m\}$
\\\qquad $\Phi\gets$ Specification; $\epsilon\gets$ Error bound for the expected features;
\\\qquad $\sigma, \alpha\in(0, 1)\gets$ Error bound $\sigma$ and step length $\alpha$ for the parameter $k$; 
\State \textbf{Initialization}:
\\\qquad \textbf{If} $||\mu_E-\mu_{\pi_0}||_2\leq \epsilon$, {\bf then return} $\pi_0$ \Comment{$\pi_0$ is the \textbf{initial safe policy}}
\\\qquad $\Pi_S\leftarrow\{\pi_0\}$, $CEX\leftarrow\emptyset$ \Comment{Initialize candidate and counterexample set}
\\\qquad $inf\leftarrow0, sup\leftarrow1, k\leftarrow sup$ \Comment{Initialize multi-optimization parameter $k$}
\\\qquad $\pi_1\leftarrow$ Policy learnt from $\mu_E$ via apprenticeship learning
\State \textbf{Iteration $i\ (i\geq 1)$}:
\\\qquad\textbf{Verifier:}
\\\qquad\qquad status $\gets Model\_Checker(M,\pi_i, \Phi)$
\\\qquad\qquad \textbf{If} status = SAT, \textbf{then go to Learner}
\\\qquad\qquad \textbf{If} status = UNSAT
\\\qquad\qquad\qquad $cex_{\pi_i}\gets Counterexample\_Generator(M,\pi_i,\Phi)$
\\\qquad\qquad\qquad Add $cex_{\pi_i}$ to $CEX$ and solve $\mu_{cex_{\pi_i}}$, \textbf{go to Learner} 
\\\qquad\textbf{Learner:}
\\\qquad\qquad \textbf{If} status = SAT
\\\qquad\qquad\qquad \textbf{If} $||\mu_E-\mu_{\pi_i}||_2\leq \epsilon$, {\bf then return} $\pi_i$
\\\Comment{Converge. $\pi_i$ is {$\epsilon$-close} to $\pi_E$}
\\\qquad\qquad\qquad Add $\pi_i$ to $\Pi_S$, $inf \leftarrow k, k \leftarrow sup$\Comment{Update $\Pi_S$, $inf$ and reset $k$}
\\\qquad\qquad \textbf{If} status = UNSAT
\\\qquad\qquad\qquad \textbf{If} $|k - inf|\leq\sigma$, {\bf then return} $\pi^*\leftarrow\underset{{\pi}\in \Pi_S}{argmin}||\mu_E - \mu_{{\pi}}||_2$\\\Comment{Converge. $k$ is too close to its lower bound. }
\\\qquad\qquad\qquad $k \leftarrow \alpha\cdot inf + (1 - \alpha)k$ 
\Comment{Decrease k to learn for safety}
\\\qquad\qquad $\omega_{i+1} \leftarrow arg\underset{\omega}{max}\underset{\pi\in\Pi_S, \tilde{\pi}\in\Pi_S, cex\in CEX}{\min}\ \omega^T(k(\mu_E - \mu_{\pi})+(1-k)(\mu_{\tilde{\pi}}  - \mu_{cex}))$
\\\Comment{Note that the multi-objective optimization function recovers AL when $k=1$} 
\\\qquad\qquad $\pi_{i+1}, \mu_{\pi_{i+1}}\gets$ Compute the optimal policy $\pi_{i+1}$ and its expected features $\mu_{\pi_{i+1}}$ for the MDP $M$ with reward $R(s)=\omega_{i+1}^T f(s)$
\\\qquad{\bf Go to next iteration}
\end{algorithmic}
\label{algo1}
\end{algorithm}

Algorithm 1 describes CEGAL in detail. 
With a constant $sup=1$ and a variable $inf\in[0, sup]$ for the upper and lower bounds respectively, the learner determines the value of $k$ within $[inf, sup]$ in each iteration depending on the outcome of the verifier and uses $k$ in solving (\ref{eq:sec5_1}) in line 27. 
Like most nonlinear optimization algorithms, this algorithm requires an initial guess, which is an initial safe policy $\pi_0$ to make $\Pi_S$ nonempty. A good initial candidate would be the maximally safe policy for example obtained using PRISM-games~{\cite{Kwiatkowska2017}}. 
Without loss of generality, we assume this policy satisfies $\Phi$.
%Given the PCTL formula $\psi$ used in $\Phi$, such as $\psi::=\true\ \until^{\leq t}\ \texttt{unsafe}$ in (\ref{eq:example-spec}), PRISM-games solves $min~P_{=?}[\psi]$ and returns a \textcolor{red}{safest} policy that satisfies the minimization. 
%If the policy satisfies $\Phi$, then $\Phi$ is proved to be satisfiable and the policy can be used as $\pi_0$ to start the iteration. 
Suppose in iteration $i$, an intermediate policy $\pi_i$ learnt by the learner in iteration $i-1$ is verified to satisfy $\Phi$, then we increase $inf$ to $inf=k$ and reset $k$ to $k=sup$ as shown in line 22. 
If $\pi_i$ does not satisfy $\Phi$, then we reduce $k$ to $k=\alpha\cdot inf + (1 - \alpha)k$  as shown in line 26 where $\alpha\in(0, 1)$ is a step length parameter. 
If $|k-inf|\leq\sigma$ and $\pi_i$ still does not satisfy $\Phi$, the algorithm chooses from $\Pi_S$ a best safe policy $\pi^*$ which has the smallest margin to $\pi_E$ as shown in line 24. If $\pi_i$ satisfies $\Phi$ and is {$\epsilon$-close} to $\pi_E$, the algorithm outputs $\pi_i$ as show in line 19. For the occasions when $\pi_i$ satisfies $\Phi$ and $inf = sup = k = 1$, solving (\ref{eq:sec5_1}) is equivalent to solving (\ref{eq:sec1_1}) as in the original AL algorithm.
%%\vspace{-4mm}
\begin{algorithm}[htb]
\caption{Initial Safe Policy Generation}
\begin{algorithmic}[1]
%\State \textbf{Input}:
%\\\qquad$M \gets$ MDP model
%\\\qquad$\mu_E\gets$ Expert's expected features; $\Phi\gets$ specification; $\epsilon\gets$ error bound.
%\\\qquad$\Pi_{CEX}=\{\}$\Comment{Counterexample set initialized to be an empty set}
%\\\qquad$\pi^*=Null\gets$ Policy to be output, preset to be Null
%\\\qquad$\mu^*=Null\gets$ Expected feature of $\pi^*$, preset to be Null
%\\\qquad ${CEX}=\emptyset \gets$ Set of counterexample features
%\\\qquad $\Pi_{S}=\emptyset \gets$ Set of safe policy features
%\\\qquad $\Pi=\emptyset \gets$ Set of policy features
%\\\qquad $SafePolicyGen() \gets$ Safe policy generator
%\\\qquad\qquad
\State \textbf{Initialize}: 
\\\qquad$\pi_0\gets$Any arbitrary policy  \Comment{Can be any policy, e.g. the policy learnt via AL}  
\\\qquad$\epsilon\gets$ Error bound for the expected features
\\\qquad$CEX\gets\emptyset$
%\\\qquad\qquad
\State \textbf{Iteration}: 
\\\qquad In iteration $i\geq1$, get $\pi_i$ from last iteration
\\\qquad\textbf{Verifier:}
\\\qquad\qquad status $\gets Model\_Checker(M,\pi_i, \Phi)$
\\\qquad\qquad \textbf{If} status = SAT, \textbf{then go to Learner}
\\\qquad\qquad \textbf{If} status = UNSAT
\\\qquad\qquad\qquad $cex_{\pi_i}\gets Counterexample\_Generator(M,\pi_i,\Phi)$
\\\qquad\qquad\qquad Add $cex_{\pi_i}$ to $CEX$ and solve $\mu_{cex_{\pi_i}}$, \textbf{go to Learner} 
\\\qquad\textbf{Learner:}
\\\qquad\qquad \textbf{If} status = SAT
\\\qquad\qquad\qquad Return $\pi_i$\Comment{End iteration once a safe policy is found}
\\\qquad\qquad \textbf{If} status = UNSAT
\\\qquad\qquad\qquad \textbf{If} $|CEX|>1$ and $\|\frac{\underset{cex \in |CEX|}{\sum}\mu_{cex}\ \ }{|CEX|}-\frac{\mu_{cex_i} + \underset{cex \in |CEX|}{\sum}\mu_{cex}\ \ }{1 + |CEX|}\|_2\leq\epsilon$
\\\qquad\qquad\qquad\qquad  \textbf{Then} return Null\\\Comment{Fail to find a safe policy since adding a new counterexample hardly changes the centroid of existing counterexample cluster.}
%\\\qquad\qquad $\omega_{i+1} = \underset{\omega}{argmax} \underset{\mu\in \Pi_{CEX}}{min}\ (-\omega^T\mu)$
\\\qquad\qquad $\omega_{i+1} = arg\max_{\omega} \min_{cex\in CEX}\ (-\omega^T\mu_{cex})$
\\\qquad\qquad $\pi_{i+1}\gets$ Optimal policy of $\omega_{i+1}$

\\\qquad Go to next iteration

\end{algorithmic}
\end{algorithm}
%%\vspace{-4mm}

\begin{remark}
The initial policy $\pi_0$ does not have to be maximally safe, although such a policy can be used to verify if $\Phi$ is  satisfiable at all. 
Naively safe policies often suffice for obtaining a safe and performant output at the end. Such a policy can be obtained easily in many settings, e.g., in the grid-world example one safe policy is simply staying in the initial cell. 
In both cases, $\pi_0$ typically has very low performance since satisfying $\Phi$ is the only requirement for it. An initial safe policy can also be generated (with no guarantee) by solving (\ref{eq:sec4_2}) iteratively as shown in Algorithm 2.
%We also note that apprenticeship learning itself does not guarantee safety for the final output even if we initialize it with a safe policy.   
\end{remark}

\begin{theorem}
%If the initial policy $\pi_0$ satisfies $\Phi$, then Algorithm~\ref{algo1} is guaranteed to output a policy $\pi^*$, such that (1) $\pi^*$ satisfies $\Phi$, and (2) the margin between the expected features $ \mu_{\pi^*}$ of $\pi^*$ and the expert's features $\mu_E$ has no smaller margin to expert's features $\mu_E$ than the expected features $\mu_{\pi_0}$ of the initial policy $\pi_0$, i.e. $\|\mu_E - \mu_{\pi^*}\|_2\leq\|\mu_E - \mu_{\pi_0}\|_2$. 
Given an initial policy $\pi_0$ that satisfies $\Phi$, Algorithm~\ref{algo1} is guaranteed to output a policy $\pi^*$, such that (1) $\pi^*$ satisfies $\Phi$, and 
(2) the performance of $\pi^*$ is at least as good as that of $\pi_0$ when compared to $\pi_E$, i.e. $\|\mu_E - \mu_{\pi^*}\|_2\leq\|\mu_E - \mu_{\pi_0}\|_2$. 
\end{theorem}
\noindent{\it Proof sketch. }%\revise{???} 
The first part of the guarantee can be proven by case splitting. Algorithm~\ref{algo1} outputs $\pi^*$ either when $\pi^*$ satisfies $\Phi$ and is {$\epsilon$-close} to $\pi_E$, or when $|k-inf|\leq \sigma$ in some iteration. 
In the first case, $\pi^*$ clearly satisfies $\Phi$. 
In the second case,  
\begin{equation}
(\pi^*=\underset{{\pi}\in\Pi_S}{argmin}||\mu_E - \mu_{{\pi}}||_2)\wedge(\pi\ satisfies\ \Phi,\ \forall\pi\in\Pi_S)
\end{equation}
is always true. Hence $\pi^*$ always satisfies $\Phi$. For the second part of the guarantee, the initial policy $\pi_0$ itself is the final output if
\begin{equation}
(\pi_0\ satisfies\ \Phi)\wedge(||\mu_E-\mu_{\pi_0}||_2\leq\epsilon). 
\end{equation}
Otherwise, $\pi_0$ is added to $\Pi_S$ if it satisfies $\Phi$.  
During the iteration, since a safe policy $\epsilon$-close to $\mu_E$ won't be added to $\Pi_S$ but directly be returned, then
\begin{equation}
(||\mu_E-\mu_{\pi}||_2\geq\epsilon, \forall\pi\in\Pi_S)\wedge\big((\pi^*=\underset{{\pi}\in\Pi_S}{argmin}||\mu_E - \mu_{{\pi}}||_2)\big)\vee(||\mu_E-\mu_{\pi^*}||_2\leq\epsilon)\big) 
\end{equation}
is always true. Hence $\|\mu_E -  \mu_{\pi^*}\|_2\leq\|\mu_E - \mu_{\pi_0}\|_2$ is always true. 
\noindent

{\it Discussion.} In the worst case CEGAL will return the initial safe policy. However, this can be because a policy that simultaneously satisfies $\Phi$ and is $\epsilon$-close to the expert's demonstrations does not exist. Comparing to AL which offers no safety guarantee and finding the maximally safe policy which has very poor performance, CEGAL provides a principled way of guaranteeing safety while retaining performance.

\noindent
