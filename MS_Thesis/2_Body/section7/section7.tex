In this chapter we have posed a SafeAL problem in \ref{sec:section3} with a motivation example and proposed to use max-margin principle to address it. The algorithm discussed in \ref{sec:section4} formulates the max-margin principle with a  multi-objective optimization problem and solves it with a weight-sum approach. The algorithm resembles AL algorithm from~\cite{Abbeel:2004:ALV:1015330.1015430} in generating candidate policies iteratively. However, given a quantitative safety specification, our algorithm provides guarantee of the safety of the output policy. We also analyzed the the performance guarantee of the final output as well as the convergence of the iteration.

Nonetheless, there are presumptions and limitations in the algorithm. The first is that the algorithm is model-based as the AL algorithm in~\cite{Abbeel:2004:ALV:1015330.1015430}, which means that the environment can be modeled as an MDP with known transition function. In robotics control tasks, this algorithm requires that the dynamics must be known beforehand. The second is that, as AL algorithm, in every iteration a policy must be solved optimally with respect to a reward function. This requires a backward pass over the entire state space in every iteration. The third is the scalability of probabilistic model checking and counterexample generation. It is still an open question. Another limitation is that this algorithm doesn't guarantee the global optimum of the final output and the convergence doesn't depend on optimum of learnt policy but on the multi-objective parameter. However, we will use experiments to evaluate the effectiveness of the algorithm.