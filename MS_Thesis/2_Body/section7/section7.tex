In this chapter we have posed a SafeAL problem in \ref{sec:section3} with a motivation example and proposed to use max-margin separation principle to address it. The algorithm discussed in \ref{sec:section4} formulates the max-margin separation principle with a  multi-objective optimization problem and solves it with a weight-sum approach. The algorithm resembles AL algorithm from~\cite{Abbeel:2004:ALV:1015330.1015430} in generating candidate policies iteratively. However, given a satisfiable safety specification described in PCTL, our algorithm provides guarantee of the safety of the output policy. We also analyzed the the performance guarantee of the final output as well as the termination of the iteration.

Nonetheless, there are assumptions and limitations in the algorithm. The first is that the algorithm is model-based as the AL algorithm in~\cite{Abbeel:2004:ALV:1015330.1015430}, which requires that the environment can be modeled as an MDP with known transition function. In robotics control tasks, this algorithm requires that the dynamics must be known beforehand. The second is that, as AL algorithm, in every iteration a policy must be solved optimally with respect to a reward function. This requires using reinforcement learning algorithm over the entire state space in every iteration. The third is that the scalability of probabilistic model checking and counterexample generation is still an open question. Another limitation is that this algorithm does not guarantee the global optimality and the termination does not depend on optimum of learnt policy but on the multi-objective parameter. 

In the next chapter, we will use experiments to evaluate the effectiveness of the algorithm.