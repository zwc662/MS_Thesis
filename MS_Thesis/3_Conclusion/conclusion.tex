\chapter{Conclusions}
\label{chapter:Conclusions}
\thispagestyle{myheadings}

% set this to the location of the figures for this chapter. it may
% also want to be ../Figures/2_Body/ or something. make sure that
% it has a trailing directory separator (i.e., '/')!
\graphicspath{{3_Conclusion/Figures/}}

\section{Summary of the thesis}
In this thesis, a counterexample-guided approach is proposed for combining formal verification with apprenticeship learning to ensure safety of the learning outcome. By giving a safety specification and adding a verification oracle to the original apprenticeship learning algorithm, it is guaranteed that only a policy that satisfies the safety specification will be output. Furthermore, when a learnt policy is verified to violate the safety specification, a counterexample, which is a proof of the violation, will be extracted from the policy. Without having to risk deploying the unsafe policy in the field, a counterexample can be regarded as a set of negative examples. The approach in this thesis makes novel use of the counterexamples to steer the policy search process by formulating the problem as a multi-objective optimization problem. By using an adaptive weight approach, the multi-objective optimization problem is solved iteratively. The multi-objective weight parameter is updated according to the learnt policy in each iteration. The algorithm is guaranteed to converge when the multi-objective weight parameter converges to a certain value. The experimental results indicate that the proposed approach can guarantee safety and retain performance for a set of benchmarks including examples drawn from OpenAI Gym.

However, shortages exist in theories. Firstly, this algorithm lacks the guarantee of finding a safe policy that performs as well as expert policy. Although it is common sense that being safe can sometimes conflicts with having high performance, a boundary to leverage between different objects would be beneficial. More specifically, when solving the multi-objective optimization problem, although an adaptive weight sum approach is employed, the algorithm doesn't end up with the Pareto frontier. Although the linear constraints ensures the dominance of the optimal $\pi$, $\hat{\pi}$ and $cex$ in individual iteration, more candidate policies and counterexamples will be found as the iteration continues. Thus, the dominance in one iteration may not hold in the future iteration. Another shortage is the gap between the margin of policy features and margin of the true policy performance. In OpenAI gym environments, where performance can be quantified, a learnt policy that is {$\epsilon$-close} to the expert policy doesn't necessarily have performance quantitatively comparable with expert policy. Whereas, this requires feature engineering as well as more accurate transition function, which is out of the scope of this thesis. The last shortage is in the convergence of algorithm. By now, the convergence is fully based on the multi-objective parameter, which makes no guarantee in global optimum of the solved policy.

\section{Future Work}
In the future, firstly we plan to deploy framework in robotic control problems that are more complex than the OpenAI gym environments, such as ROS\footnote{\url{http://wiki.ros.org/ROS/Tutorials/InstallingandConfiguringROSEnvironment}} environment and Baxter\footnote{\url{http://www.rethinkrobotics.com/baxter/}} robot. Secondly, we will test the scalability of the current verification techniques utilized in our prototype. Different methods in policy verification and counterexample generation will also be considered in case of necessity of improving the performance of the framework. In addition, we will try to cover the theoretical shortages in the CEGAL algorithm. For instance, we will consider using gradient based method, rather than iteration based method, to solve apprenticeship learning problem or other imitation learning problems. Meanwhile, we will try to find new ways to incorporate counterexample in the learning process.