\chapter{Conclusions}
\label{chapter:Conclusions}
\thispagestyle{myheadings}

% set this to the location of the figures for this chapter. it may
% also want to be ../Figures/2_Body/ or something. make sure that
% it has a trailing directory separator (i.e., '/')!
\graphicspath{{3_Conclusion/Figures/}}

\section{Summary of the thesis}
In this thesis, a counterexample-guided approach is proposed for combining probabilistic model checking with apprenticeship learning to ensure safety of the learning outcome. By giving a safety specification and adding a verification oracle to the original apprenticeship learning algorithm, it is guaranteed that only a policy that satisfies the safety specification will be output. Furthermore, when a learnt policy is verified to violate the safety specification, a counterexample, which is a proof of the violation, will be extracted from the policy without risking deploying the unsafe policy in the field. Regarding counterexample as a negative example, the approach in this thesis makes novel use of the counterexamples to steer the policy search process by formulating the problem as a multi-objective optimization problem. By using an adaptive weight approach, the multi-objective optimization problem is solved iteratively and the multi-objective weight parameter is updated according to the learnt policy in each iteration. The algorithm is guaranteed to converge when the multi-objective weight parameter converges to a certain value. The experiment results indicate that the proposed approach can guarantee safety and retain performance for a set of benchmarks including examples drawn from OpenAI Gym.

However, the approach still has shortages in theories. One is lack of guarantee in finding a safe policy that performs as well as expert policy. Although it is natural that being safe can sometimes conflicts with high performance, I still need to find a boundary to leverage between different objects. For instance, in the multi-objective optimization problem, I used adaptive weighted sum approach. However, I didn't find the Pareto frontier for different $k$. Although I use constraints to ensure the dominance of the optimal $\pi$, $\hat{\pi}$ and $cex$ in individual iteration, more candidate policies and counterexamples will be found as the iteration continues. Thus, the dominance in one iteration may not hold in the future iteration. Another one shortage is the gap between the margin of policy features and margin of the true policy performance. In OpenAI gym environments, where performance can be quantified, a learnt policy that is {$\epsilon$-close} to the expert policy doesn't necessarily have performance comparable with expert policy. Whereas, this requires feature engineering, which is out of scope of this thesis. The last shortage s in the convergence of algorithm. By now, the convergence is fully based on the multi-objective parameter, which makes no guarantee in global optimum of the solved policy.

\section{Future Work}
In the future, firstly I'll try to deploy the algorithm and framework in realistic robotic control problems that have larger scale and are more complex than the OpenAI gym environments. Other than application, I would like to cover the theoretical shortages. For instance, I'll try to use gradient based method, rather than maximum margin principle, to solve apprenticeship learning problem and incorporate counterexample. Meanwhile, I would also like to explore other imitation or apprenticeship learning algorithms and extend our techniques to those settings. Last but not least, I would try different methods in policy verification and counterexample generation for substitutes to improve the scalability of the framework.

