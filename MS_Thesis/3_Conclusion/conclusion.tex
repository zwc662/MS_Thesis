\chapter{Conclusions}
\label{chapter:Conclusions}
\thispagestyle{myheadings}

% set this to the location of the figures for this chapter. it may
% also want to be ../Figures/2_Body/ or something. make sure that
% it has a trailing directory separator (i.e., '/')!
\graphicspath{{3_Conclusion/Figures/}}

\section{Summary of the thesis}
In this thesis, a counterexample-guided approach is proposed for combining formal verification with apprenticeship learning to ensure safety of the learning outcome. By giving a safety specification and adding a verification oracle to the original apprenticeship learning algorithm, it is guaranteed that only a policy that satisfies the safety specification will be output. Furthermore, when a learnt policy is verified to violate the safety specification, a counterexample, which is a proof of the violation, will be extracted from the policy. Without having to risk deploying the unsafe policy in the field, a counterexample can be regarded as a set of negative examples. The approach in this thesis makes novel use of the counterexamples to steer the policy search process by formulating the problem as a multi-objective optimization problem. By using an adaptive weight approach, the multi-objective optimization problem is solved iteratively. The multi-objective weight parameter is updated according to the learnt policy in each iteration. The algorithm is guaranteed to terminate when the multi-objective weight parameter converges to a certain value. The experimental results indicate that the proposed approach can guarantee safety and retain performance for a set of benchmarks including examples drawn from OpenAI Gym.

In addition to what have been achieved in this thesis, there are some open challenges in the theories of the algorithm. Firstly, this thesis does not guarantee finding a safe policy that performs as well as expert policy. Although it is common sense that being safe can sometimes conflicts with having high performance, finer control in leverage between different objects would be beneficial. More specifically, when solving the multi-objective optimization problem, although an adaptive weight sum approach is employed, the algorithm does not end up with the Pareto frontier. Although the linear constraints ensures the dominance of the optimal $\pi$, $\hat{\pi}$ and $cex$ in individual iteration, more candidate policies and counterexamples will be found as the iteration continues. Thus, the dominance in one iteration may not hold in the future iteration. One shortcoming of the thesis is in the termination of the algorithm which is fully based on the multi-objective parameter rather than converging to the global optimum of the solved policy. There are also challenges in  model construction which brings the gap between the margin of policy features and margin of the true policy performance. In OpenAI gym environments, where performance can be quantified, a learnt policy that is {$\epsilon$-close} to the expert policy does not necessarily have performance quantitatively comparable with expert policy. Whereas, this requires dedicated feature design for the reward function as well as more accurate transition function, which is out of the scope of this thesis.

\section{Future Work}
In the future, firstly we plan to deploy our framework in tasks more complex than the OpenAI gym environments, such as ROS\footnote{\url{http://wiki.ros.org/ROS/Tutorials/InstallingandConfiguringROSEnvironment}} environment and Baxter\footnote{\url{http://www.rethinkrobotics.com/baxter/}} robot. 

Secondly, we will test the scalability of the current verification techniques utilized in our prototype. Different methods in policy verification and counterexample generation will also be considered in case of necessity of improving the efficiency of the framework. For instance, we are considering applying statistical model checking~\cite{10.1007/978-3-642-19829-8_10},~\cite{Younes:2006:SPM:1182767.1182770},~\cite{henriques2012statistical} in large scale learning tasks where the state space may be too large for symbolic model checking techniques.

In addition, we will concentrate on addressing the theoretical shortcomings in the CEGAL algorithm. For instance, we will consider using gradient based method as in~\cite{neu2012apprenticeship}, rather than iteration based method, to solve apprenticeship learning problem or other imitation learning problems, such as the model-free method in~\cite{ho2016model}. Meanwhile, we will investigate new ways to incorporate counterexample in the learning process.