\chapter{Introduction}
\label{chapter:Introduction}
\thispagestyle{myheadings}
{\bf AI safety} has been one of the major topics in artificial intelligence researches. As AI technologies move toward broader deployment, concerns about unintended consequences of widespread adoption have been raised. When exposed to the full complexity of the human environment, an AI agent should not only be able to finish the task assigned by human but also assure that its probability of performing unsafe actions is at least below some fairly low level. This is especially significant in safety critical areas. As highlighted in \cite{AmodeiOSCSM16}, if the objective function of an AI agent is wrongly specified, then maximizing that objective function may lead to harmful results. In addition, if an agent focuses only on accomplishing a specific task and ignore other aspects, such as safety constraints, of the environment, it may also perform unsafe behavior. But among various reasons why an agent learns an unsafe policy, the direct and intuitive cause is that the agent lacks for awareness of unsafe situations.  

{\bf Apprenticeship learning (AL)} approach developed by Abbeel and Ng~\cite{Abbeel:2004:ALV:1015330.1015430} is one form of learning from demonstration where an agent tries to recover an expert's strategy by observing and learning from the expert's demonstration. This approach uses {\it inverse reinforcement learning}~\cite{Ng:2000:AIR:645529.657801} technique in which it's assumed that expert makes decisions optimally in the environment. Apprenticeship Learning bypasses the explicit search of reward function as in reinforcement learning and directly recovers the expert's policy by matching the features of the expert's demonstration. However, the expert often can only demonstrate how the task works but not how the task may fail. This is because failure may cause irrecoverable damages to the system such as crashing a vehicle. In general, the lack of ``negative examples" can cause a heavy bias in how the learning agent constructs the reward estimate. In fact, even if all the demonstrations are safe, the agent may still end up learning an unsafe policy.

In this dissertation, we explore the following thesis:

\emph {
By learning from safe behaviors that are provided by human demonstration, as well as unsafe behaviors that are generated by a verification oracle, an agent can induce a policy that both resembles human and is provably safe. 
} 

More specifically, we look into the safety problems in apprenticeship learning algorithm. Considering a safety specification expressed in probabilistic computation tree logic (PCTL)~\cite{Hansson1994}, we employ a verification-in-the-loop approach by embedding PCTL model checking as a safety checking mechanism inside the learning phase of AL. Inspired by the work on formal inductive synthesis~\cite{jha-ai2017} and counterexample-guided inductive synthesis~\cite{CEGIS}, we incorporate formal verification in apprenticeship learning and propose a framework that uses the verification results to  inductively improve the learnt policy. 
More specifically, when a learnt policy does not satisfy the PCTL formula, we leverage counterexamples generated by the model checker to steer the policy search in AL. 
In essence, counterexample generation can be viewed as supplementing negative examples for the learner. 
Thus, the learner will try to find a policy that not only imitates the expert's demonstrations but also stays away from the failure scenarios as captured by the counterexamples. 


\section{Contributions}
% List of contributions
In summary, we make the following contributions in this paper. 
\begin{itemize}
\item We propose a novel framework for incorporating formal safety guarantees in learning from demonstrations.
\item We develop a novel algorithm called \underline{C}ounter\underline{E}xample \underline{G}uided \underline{A}pprenticeship \underline{L}earning (CEGAL) that combines probabilistic model checking with the optimization-based approach of apprenticeship learning. 
\item We demonstrate that the proposed approach can guarantee safety for a set of case studies and attain comparable or even better performance compared to using apprenticeship learning alone.
\end{itemize}


\section{Related Work}
A taxonomy of AI safety problems is given in \cite{AmodeiOSCSM16} where the issues of misspecified objective or reward and insufficient or poorly curated training data were highlighted. There have been several efforts attempting to address these issues from different angles. The problem of {\it safe exploration} is studied in \cite{moldovan2012safe} and \cite{DBLP:journals/corr/HeldMZSA17}. In particular, the latter work proposes to add a safety constraint on amount of damage, to the optimization problem so that the optimal policy can maximize the reward without violating the limit on the expected damage. An obvious shortcoming of this approach is that actual failures will have to occur so that the constraint can be properly decided.

Formal methods have been applied to the problem of AI safety. 
In \cite{gillulay2011guaranteed}, the authors propose to combine machine learning and reachability analysis for dynamical models to achieve high performance and guarantee safety. In \cite{DBLP:journals/corr/SadighKCSS14}, the authors propose to use formal specification to synthesize a control policy for reinforcement learning. Recently, the problem of {\it safe reinforcement learning} was explored in \cite{DBLP:journals/corr/abs-1708-08611} where a monitor (called shield) is used to enforce temporal logic properties by providing a list of safe actions each time the agent makes a decision so that the temporal property is preserved. 
In \cite{junges2016safety}, the authors also propose an approach for controller synthesis for reinforcement learning by using an SMT-solver is used to find a scheduler (policy) so that it satisfies both a probabilistic reachability property and an expected cost property. 
In \cite{mason2017assured}, a so-called abstract Markov decision process (AMDP) model of the environment is first built and PCTL model checking is then used to check the satisfiability of safety specification.
Our work is similar to these in spirit in the application of formal methods. However, while the concept of AL is closely related to reinforcement learning, an agent in the AL paradigm needs to learn a policy from demonstrations without knowing the reward function.
A related safety problem in verification is the faithfulness of the model when it is learned from data. 
In \cite{puggelli-cav13}, the authors propose a convex-MDP model for capturing uncertainties in the transition probabilities of an MDP as convex sets and propose a polynomial-time algorithm for verifying PCTL properties on these convex-MDPs. 

Among imitation or apprenticeship learning methods, margin based algorithms \cite{Abbeel:2004:ALV:1015330.1015430}, \cite{Ng:2000:AIR:645529.657801}, \cite{Ratliff:2006:MMP:1143844.1143936} try to maximize the margin between the expert's policy and all learnt policies until the one with the smallest margin is produced. The apprenticeship learning algorithm in \cite{Abbeel:2004:ALV:1015330.1015430} was largely motivated by the support vector machine (SVM). Our algorithm makes use of this observation when using counterexamples to steer the policy search process.
Recently, the idea of learning from failed demonstrations started to emerge. 
In \cite{shiarlis2016inverse}, the authors propose an IRL algorithm that can learn from both successful and failed demonstrations. It is done by reformulating maximum entropy algorithm in \cite{Ziebart:2008:MEI:1620270.1620297} to find a policy that maximally deviates from the failed demonstrations while approaches the successful ones as much as possible. However, this entropy-based method requires obtaining many failed demonstrations and can be very costly in practice. 

Finally, our approach is inspired by the work on formal inductive synthesis~\cite{jha-ai2017} and counterexample-guided inductive synthesis (CEGIS)~\cite{CEGIS}. These frameworks typically combine a constraint-based synthesizer with a verification oracle. In each iteration, the agent refines her hypothesis (i.e. generates a new candidate solution) based on counterexamples provided by the oracle. Our approach can be viewed as an extension of CEGIS where the objective is not just functional correctness but also meeting certain learning criteria. 

\section{Outline}
% Organization of the paper
The rest of the paper is organized as follows. 
Chapter~\ref{sec:background} reviews background information on apprenticeship learning and PCTL model checking. 
Chapter~\ref{sec:section3} defines the safety-aware apprenticeship learning problem and gives an overview of our approach. 
Chapter~\ref{sec:section4} illustrates the counterexample-guided learning framework. 
Chapter~\ref{sec:section5} and~\ref{sec:section6} describes the proposed algorithm in detail. 
Chapter~\ref{sec:exp} presents a set of experimental results demonstrating the effectiveness of our approach. 
%Section~\ref{sec:conclu} concludes and offers future directions. 

